{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing for Inference Speed\n",
    "\n",
    "This notebook demonstrates techniques for optimizing a trained language model for faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.45.1\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model\n",
    "\n",
    "We'll use a GPT-2 model for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Post-Training Quantization to INT8\n",
    "\n",
    "We'll use PyTorch's dynamic quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"Model quantized to INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Knowledge Distillation (Simulated)\n",
    "\n",
    "In practice, knowledge distillation involves training a smaller model to mimic a larger one. For this notebook, we'll simulate a distilled model by using a smaller version of GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "distilled_model.to(device)\n",
    "print(\"Loaded distilled model (DistilGPT-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Efficient Attention Mechanism\n",
    "For this example, we'll use the Flash Attention mechanism using PyTorch's `scaled_dot_product_attention` function, which under the hood uses Flash Attention when enabled `torch.backends.cuda.enable_flash_sdp(True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.attention import sdpa_kernel\n",
    "import torch.backends.cuda\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.split_size = self.embed_dim\n",
    "        \n",
    "        self.c_attn = nn.Linear(self.embed_dim, 3 * self.embed_dim)\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.attn_dropout = config.attn_pdrop\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, attn_head_size):\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, attn_head_size):\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
    "        qkv = self.c_attn(hidden_states)\n",
    "        query, key, value = qkv.split(self.split_size, dim=2)\n",
    "        \n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "        \n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            key = torch.cat((past_key, key), dim=-2)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "        \n",
    "        if use_cache is True:\n",
    "            present = (key, value)\n",
    "        else:\n",
    "            present = None\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_mask = attention_mask.to(dtype=query.dtype)  # fp16 compatibility\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(query.dtype).min\n",
    "\n",
    "        # Use scaled_dot_product_attention with Flash Attention\n",
    "        with sdpa_kernel():\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                query, key, value,\n",
    "                attn_mask=attention_mask,\n",
    "                dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "                is_causal=True,\n",
    "                need_weights=output_attentions\n",
    "            )\n",
    "        \n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "        \n",
    "        outputs = (attn_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (None,)  # We don't have attention weights due to using scaled_dot_product_attention\n",
    "        \n",
    "        return outputs  # a, present, (attentions)\n",
    "\n",
    "# Enable Flash Attention globally\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)\n",
    "\n",
    "# Function to check if Flash Attention is being used\n",
    "def is_using_flash_attention():\n",
    "    x = torch.randn(2, 4, 8, 16, device='cuda')\n",
    "    with sdpa_kernel():\n",
    "        y = F.scaled_dot_product_attention(x, x, x)\n",
    "    return y.is_contiguous()\n",
    "\n",
    "# Modified function to replace attention layers\n",
    "def replace_attention_layers(module, config):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, transformers.models.gpt2.modeling_gpt2.GPT2Attention):\n",
    "            setattr(module, name, FlashAttention(config))\n",
    "        else:\n",
    "            replace_attention_layers(child, config)\n",
    "\n",
    "# Load the model and its configuration\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "config = model.config\n",
    "\n",
    "# After loading the model, replace the attention layers\n",
    "replace_attention_layers(model, config)\n",
    "print(\"Replaced attention layers with FlashAttention\")\n",
    "\n",
    "# Verify Flash Attention usage\n",
    "print(f\"Using Flash Attention: {is_using_flash_attention()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimize Model Architecture\n",
    "\n",
    "We'll replace LayerNorm with RMSNorm for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "def replace_layernorm_with_rmsnorm(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            setattr(model, name, RMSNorm(module.normalized_shape[0]))\n",
    "        else:\n",
    "            replace_layernorm_with_rmsnorm(module)\n",
    "\n",
    "replace_layernorm_with_rmsnorm(model)\n",
    "print(\"Replaced LayerNorm with RMSNorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmark Inference Speed\n",
    "\n",
    "Let's compare the inference speed of our original and optimized models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model_or_func, input_ids, num_runs=100):\n",
    "    if hasattr(model_or_func, 'eval'):\n",
    "        model_or_func.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = model_or_func(input_ids)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = model_or_func(input_ids)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    return (end_time - start_time) / num_runs\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"Original model inference time:\")\n",
    "original_time = benchmark_inference(model, input_ids)\n",
    "print(f\"{original_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nQuantized model inference time:\")\n",
    "quantized_time = benchmark_inference(quantized_model, input_ids)\n",
    "print(f\"{quantized_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nDistilled model inference time:\")\n",
    "distilled_time = benchmark_inference(distilled_model, input_ids)\n",
    "print(f\"{distilled_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\nFlash Attention model inference time:\")\n",
    "flash_time = benchmark_inference(model, input_ids)\n",
    "print(f\"{flash_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to ONNX for Optimized Inference\n",
    "\n",
    "Finally, let's export our optimized model to ONNX format for even faster inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX\n",
    "dummy_input = torch.randint(0, 50000, (1, 512), dtype=torch.long).to(device)\n",
    "torch.onnx.export(model, dummy_input, \"optimized_gpt2.onnx\",\n",
    "                  input_names=['input_ids'],\n",
    "                  output_names=['logits'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                                'logits': {0: 'batch_size', 1: 'sequence'}},\n",
    "                  opset_version=12)\n",
    "\n",
    "# Create an ONNX inference session\n",
    "import onnxruntime as ort\n",
    "ort_session = ort.InferenceSession(\"optimized_gpt2.onnx\")\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "def onnx_inference(session, input_ids):\n",
    "    ort_inputs = {'input_ids': input_ids.cpu().numpy()}\n",
    "    ort_outputs = session.run(None, ort_inputs)\n",
    "    return ort_outputs[0]\n",
    "\n",
    "print(\"\\nONNX model inference time:\")\n",
    "onnx_time = benchmark_inference(lambda x: onnx_inference(ort_session, x), input_ids)\n",
    "print(f\"{onnx_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
