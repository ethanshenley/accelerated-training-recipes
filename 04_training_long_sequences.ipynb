{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Very Long Sequences\n",
    "\n",
    "This notebook demonstrates techniques for training language models on very long sequences, which is crucial for tasks requiring extended context understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.nn.attention import sdpa_kernel\n",
    "from torch.backends.cuda import sdp_kernel, SDPBackend\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model and Dataset\n",
    "\n",
    "We'll use a Longformer model, which is designed for long sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/longformer-base-4096\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load a dataset with long documents\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable FlashAttention or other SDP kernels\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "# Define the replacement function\n",
    "def replace_attention_layers(module):\n",
    "    if isinstance(module, transformers.models.longformer.modeling_longformer.LongformerSelfAttention):\n",
    "        # Ensure we're using the most optimized attention\n",
    "        with torch.backends.cuda.sdp_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "            module.query_global = torch.nn.Linear(module.embed_dim, module.num_heads * module.head_dim, bias=module.query_global.bias is not None)\n",
    "            module.key_global = torch.nn.Linear(module.embed_dim, module.num_heads * module.head_dim, bias=module.key_global.bias is not None)\n",
    "            module.value_global = torch.nn.Linear(module.embed_dim, module.num_heads * module.head_dim, bias=module.value_global.bias is not None)\n",
    "    for child in module.children():\n",
    "        replace_attention_layers(child)\n",
    "\n",
    "# Ensure you have the model initialized\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Replace the attention layers in the model\n",
    "replace_attention_layers(model)\n",
    "print(\"Optimized attention layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Gradient Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "print(\"Enabled gradient checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Curriculum Learning\n",
    "\n",
    "We'll implement a simple curriculum learning strategy, gradually increasing sequence length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum learning variables\n",
    "max_length = 4096\n",
    "min_length = 1024\n",
    "num_epochs = 3\n",
    "\n",
    "def get_sequence_length(epoch):\n",
    "    return min(max_length, min_length + (max_length - min_length) * epoch // num_epochs)\n",
    "\n",
    "class CurriculumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, tokenizer, epoch):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = get_sequence_length(epoch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', max_length=self.seq_length, return_tensors='pt')\n",
    "        \n",
    "        # For MLM, labels are the same as input_ids, but ignore padding tokens (-100)\n",
    "        encoding['labels'] = encoding['input_ids'].clone()\n",
    "        encoding['labels'][encoding['input_ids'] == self.tokenizer.pad_token_id] = -100  # Mask padding tokens\n",
    "        \n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "\n",
    "print(f\"Implemented curriculum learning with max length: {max_length}\")\n",
    "model.config.attention_window = [512] * model.config.num_hidden_layers\n",
    "print(f\"Set sliding window size to {model.config.attention_window[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Sliding Window Attention\n",
    "\n",
    "Longformer already implements sliding window attention, but let's ensure it's properly configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.attention_window = [512] * model.config.num_hidden_layers\n",
    "print(f\"Set sliding window size to {model.config.attention_window[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    ")\n",
    "\n",
    "# Train and evaluate for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    train_dataset = CurriculumDataset(dataset['train'], tokenizer, epoch)\n",
    "    eval_dataset = CurriculumDataset(dataset['validation'], tokenizer, epoch)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Sequence Length: {get_sequence_length(epoch)}\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate after each epoch\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference on Long Sequences\n",
    "\n",
    "After training, let's test the model on a long sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_text(prompt, max_length=4096):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "long_prompt = \"In a world where technology has advanced beyond our wildest dreams, \"\n",
    "generated_text = generate_long_text(long_prompt)\n",
    "print(f\"Generated text length: {len(generated_text.split())}\")\n",
    "print(generated_text[:500] + \"...\")  # Print the first 500 characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
